{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "$\\textbf{Introdution.}$ We have seen that the second step of statistics may be data description. It may be intricately linked to data cleaning: for instance, you realize your data needs cleaning when describing it through a histogram and ending up with absurd values.\n",
    "\n",
    "On the other hand, dimensionality reduction may serve different purposes:\n",
    "    \n",
    "- $\\textbf{Visualization:}$ if your initial data lies in a high dimensional space, you may want to visualize it\n",
    "in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ or to detect possible patterns or anomalies;\n",
    "\n",
    "- $\\textbf{Providing a suitable input:}$ if your database is so large that you cannot practically compute\n",
    "estimates with it all, dimensionality reduction is mandatory;\n",
    "\n",
    "- $\\textbf{Avoiding the curse of dimensionality:}$ this expression refers to range of issues encountered\n",
    "when dealing with data in high-dimensional spaces. Indeed, it becomes more difficult to\n",
    "detect patterns overall, as data density decreases and computational complexity increases.\n",
    "Therefore dimensionality reduction techniques also help providing better suited inputs for\n",
    "models.\n",
    "\n",
    "$\\textbf{Dataset.}$ In this session, we are first focusing on a standard dataset in machine learning: the MNIST dataset of handwritten digits. It contains 70,000 black and white $28\\times 28$ pixel images representing handwritten digits from 0 to 9. We are going to answer the following questions:\n",
    "- How to visualize MNIST images in $\\mathbb{R}^2$ in a pertinent way (ie in way that a reflects intrinsic similarities) ?\n",
    "- How to assess whether dimensionality reduction has yielded qualitatively suitable outputs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((X,Y),(enval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "X=X/np.float32(255)\n",
    "Y=Y.astype(np.int32)\n",
    "\n",
    "eval_data = eval_data/np.float32(255)\n",
    "eval_labels = eval_labels.astype(np.int32)\n",
    "\n",
    "print(\"Training data columns \"+str(X.shape[0])+\"example of size \"+str(X.shape[1])+\"*\"+str(X.shape[2]))\n",
    "print(\"Test data contains \"+str(eval_data.shape[0])+\"example of size\"+str(eval_data.shape[1])+\"*\"+str(eval_data.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Visualizing data examples: show how data look like, choose one example and show it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,X.shape[0])\n",
    "print(\"image has labels\"+str(Y[idx]))\n",
    "plt.figure()\n",
    "plt.imshow(X[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Turn data into dataframe\n",
    "Give names pixel0, ... pixel783 to your columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X,(X.shape[0],28*28))\n",
    "feat_cols = ['pixel'+str(i) for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X,columns=feat_cols)\n",
    "df['label'] = Y\n",
    "df['label'] = df['label'].apply(lambda i: str(i))\n",
    "print('Size of the dataframe is {}'.format(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Using this format, we can visualize a subsample of the digits.\n",
    "visualizate table with size $3\\times 10$, consisting of random digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation for random subsampling\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "# plot some chosen examples\n",
    "plt.gray()\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "for i in range(0,30):\n",
    "  ax = fig.add_subplot(3,10,i+1,title='Digit:'+df.loc[rndperm[i],\"label\"]) \n",
    "  ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(float))\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction through Principal Component Analysis (PCA).\n",
    "\n",
    "The aim of PCA is to turn a set of possibly correlated variables into linearly uncorrelated variables (called “principal components”) through an orthogonal transformation. The idea is that:\n",
    "- the first component ($C_1$) accounts for most of the variance in the data;\n",
    "- the second ($C_2$) accounts for the second highest variance, and is orthogonal to $C_1$;\n",
    "- etc.\n",
    "The orthogonality condition makes the data way easier to visualize, while the variance criterion ranks components by order of relative importance.\n",
    "\n",
    "In order to achieve that, the optimization procedure aims at finding a matrix of weights $W$ mapping each $x_i \\in \\mathbb{R}^d$ to a principal components score $t_{k,i}=\\langle x_i, w_k\\rangle$ inheriting the most variance from $X$, ie the first weight vector $w_1$ solves: $$\\max_{\\lvert\\lvert w\\rvert \\rvert} \\sum_i (t_{1,i})^2.$$ Other weights are retrieved by the same operation on $X$ minus its first components.\n",
    "\n",
    "This process yields a decomposition of the form $T=XW$. Using this: \n",
    "- dimensionality reduction can be performed in this context by only selecting the first $L$ components: $T_L=X W_L$;\n",
    "- a first idea for visualization in $\\mathbb{R}^2$ is to only represent the first two components.\n",
    "\n",
    "Let us try this out on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Perform PCA\n",
    "Create 2-d scatter plot with first and second dimensions of PCA, draw different dots with different colors. Use `seaborn` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result=pca.fit_transform(df[feat_cols].values)\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1]\n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variance ratio per principal component:{}'.format(pca.explained_variance_ratio_))\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(x_vars=['pca-one'],y_vars=['pca-two'],data=df,hue='label',height=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Make inference about this method\n",
    "It seems that 0s and 1s are well separated from other classes. However, for the other labels, let us try to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction through t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "On the other hand, we study t-SNE, which is a nonlinear dimensionality reduction method particularly suited for the visualization of high-dimensional data.\n",
    "\n",
    "The idea is reduce dimensionality by minimizing the distance between pairwise probabilities in the high-dimensional space and in a lower-dimensional space. It consists of two steps:\n",
    "- constructing a probability on the high dimensional object pairs: similar pairs have a high probability of being chosen, unlike dissimilar observations;\n",
    "- creating a similar probability distribution in a low dimensional space, minimizing some distance between the two.\n",
    "\n",
    "Let us try it: is it possible to apply it on all the data? Does that improve the visual performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Use t-SNE model to do dimensionality reduction and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-sne\n",
    "n_sne = 7000 #data subsampling\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2,verbose=1,perplexity=40,n_iter=300)\n",
    "tsne_result = tsne.fit_transform(df.loc[rndperm[:n_sne],feat_cols].values)\n",
    "print('t-SNE done! Time elapsed: {}seconds'.format(time.time()-time_start))\n",
    "df_tsne = df.loc[rndperm[:n_sne],:].copy()\n",
    "df_tsne['x-tsne'] = tsne_result[:,0]\n",
    "df_tsne['y-tsne'] = tsne_result[:,1]\n",
    "\n",
    "import seaborn as sns\n",
    "sns.pairplot(x_vars=['x-tsne'],y_vars=['y-tsne'],data=df_tsne,hue='label',height=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Make inference about this method\n",
    "On a subsample, we seem to have gained performance. However some classes are still quite difficult to differenciate. Let us see if we can do even better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of the two.\n",
    "\n",
    "Since PCA handles large inputs better, a solution would be to combine the two, and perform t-SNE on PCA outputs. How does that improve performance ? How far can we go in terms of components ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) use 50 dimentions with PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-sne on larger PCA\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_result_50 = pca_50.fit_transform(df[feat_cols].values)\n",
    "print('Cumulative explianed variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sne = 10000\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2,verbose=1,perplexity=40,n_iter=300)\n",
    "tsne_pca_results = tsne.fit_transform(pca_result_50[rndperm[:n_sne]])\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Implement the tSNE model and visualise results and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = None\n",
    "df_tsne = df.loc[rndperm[:n_sne],:].copy()\n",
    "df_tsne['x-tsne-pca'] = tsne_pca_results[:,0]\n",
    "df_tsne['y-tsne-pca'] = tsne_pca_results[:,1]\n",
    "\n",
    "sns.pairplot(x_vars=['x-tsne-pca'],y_vars=['y-tsne-pca'],data=df_tsne,hue='label',height=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
